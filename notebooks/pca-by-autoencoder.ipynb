{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import scienceplots\n",
    "from drcomp.autoencoder import FullyConnectedAE\n",
    "from drcomp.reducers import PCA, AutoEncoder\n",
    "from drcomp.utils.notebooks import get_dataset\n",
    "\n",
    "plt.style.use(\"science\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_linear_autoencoder(input_size: int, intrinsic_dim: int, weight_decay: int):\n",
    "    base = FullyConnectedAE(\n",
    "        input_size=input_size,\n",
    "        intrinsic_dim=intrinsic_dim,\n",
    "        include_batch_norm=False,\n",
    "        tied_weights=False,\n",
    "        encoder_act_fn=nn.Identity,\n",
    "    )\n",
    "\n",
    "    return AutoEncoder(\n",
    "        base,\n",
    "        max_epochs=50,\n",
    "        lr=0.001,\n",
    "        batch_size=128,\n",
    "        weight_decay=weight_decay,\n",
    "    )\n",
    "\n",
    "\n",
    "def plot_weights(weights, intrinsic_dim, img_size, title=None, axs=None):\n",
    "    assert weights.shape == (\n",
    "        intrinsic_dim,\n",
    "        np.prod(img_size),\n",
    "    ), f\"Weights must be of shape (intrinsic_dim, np.prod(img_size)), but got {weights.shape}.\"\n",
    "    if axs is None:\n",
    "        fig, axs = plt.subplots(nrows=3, ncols=3, figsize=(5.91, 5))\n",
    "    for ax, weight in zip(axs.flat, weights):\n",
    "        ax.imshow(weight.reshape(img_size), cmap=\"gray\")\n",
    "        ax.axis(\"off\")\n",
    "    if title is not None:\n",
    "        plt.suptitle(title)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA, Autoencoder and the method implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analytical_pca(X, intrinsic_dim):\n",
    "    \"\"\"Standard PCA implementation from scikit-learn.\"\"\"\n",
    "    pca = PCA(intrinsic_dim).fit(X)\n",
    "    loadings = pca.pca.components_\n",
    "    embedding = pca.transform(X)\n",
    "    return loadings, embedding\n",
    "\n",
    "\n",
    "def linear_autoencoder_weights(X, intrinsic_dim, weight_decay):\n",
    "    \"\"\"Return the decoder weights of a linear autoencoder.\n",
    "\n",
    "    Fits an linear autoencoder on X and returns the weights of the decoder, as well as the embedding Y.\n",
    "    \"\"\"\n",
    "    ae = get_linear_autoencoder(\n",
    "        X.shape[1], intrinsic_dim, weight_decay=weight_decay\n",
    "    ).fit(X)\n",
    "    weights = ae.module_.decoder[0].weight.data.numpy().T\n",
    "    embedding = ae.transform(X)\n",
    "    return weights, embedding\n",
    "\n",
    "\n",
    "def pca_by_autoencoder(X, weights):\n",
    "    \"\"\"PCA by the weights of a linear autoencoder.\n",
    "\n",
    "    Applies SVD to the decoder weights of a linear autoencoder and returns\n",
    "    left-singular vectors and the embedding of X.\n",
    "    \"\"\"\n",
    "    U, s, _ = np.linalg.svd(weights.T, full_matrices=False)\n",
    "    U = U[:, np.argsort(s)[::-1]]\n",
    "    embedding = X @ U\n",
    "    return U.T, embedding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load FER dataset\n",
    "X, y = get_dataset(\"FER2013\", root_dir=\"..\")\n",
    "X_train = StandardScaler(with_mean=True, with_std=True).fit_transform(X)\n",
    "\n",
    "intrinsic_dim = 9\n",
    "img_size = (48, 48, 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply the methods\n",
    "\n",
    "First, we apply regular PCA to the data and save the loadings as `loadings_analytical`.\n",
    "\n",
    "Then, we apply the PCA by linear autoencoder method of Plaut (2018) using two autoencoders.\n",
    "The method of Plaut (2018) consists of calculating the SVD of the decoder (or encoder) weight matrix of a linear autoencoder.\n",
    "The loadings are then the left-singular vectors of the SVD.\n",
    "\n",
    "Here, we use two autoencoders: the first autoencoder does not use weight decay, while the second autoencoder uses weight decay.\n",
    "We save the weights of the decoders as `weights_unreg` and `weights_reg` respectively.\n",
    "\n",
    "The left-singular vectors of the decoder matrices are saved as `loadings_by_ae_unreg` and `loadings_by_ae_reg` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analytical PCA\n",
    "loadings_analytical, embedding_pca = analytical_pca(X_train, intrinsic_dim)\n",
    "\n",
    "# unregularized autoencoder\n",
    "weights_unreg, embedding_ae_unreg = linear_autoencoder_weights(\n",
    "    X_train, intrinsic_dim, weight_decay=0\n",
    ")\n",
    "loadings_by_ae_unreg, embedding_pca_by_ae_unreg = pca_by_autoencoder(\n",
    "    X_train, weights_unreg\n",
    ")\n",
    "\n",
    "# regularized autoencoder\n",
    "weights_reg, embedding_ae_reg = linear_autoencoder_weights(\n",
    "    X_train, intrinsic_dim, weight_decay=2e-4\n",
    ")\n",
    "loadings_by_ae_reg, embedding_pca_by_ae_reg = pca_by_autoencoder(X_train, weights_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = [embedding_pca, embedding_pca_by_ae_unreg, embedding_pca_by_ae_reg]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suptitles = [\"(a)\", \"(b)\", \"(c)\"]\n",
    "\n",
    "# plot the covariance matrices of the embeddings\n",
    "fig, axes = plt.subplots(1, 3, figsize=(5.91, 2.8))\n",
    "for i, (ax, embedding) in enumerate(zip(axes, embeddings)):\n",
    "    cov = np.cov(embedding, rowvar=False)\n",
    "    ax.imshow(cov, cmap=\"gray\")\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    plt.text(0.5, -0.2, suptitles[i], transform=ax.transAxes, ha=\"center\", fontsize=11)\n",
    "fig.savefig(\"../figures/covariance-matrices.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the weights\n",
    "fig = plt.figure(figsize=(5.91, 1.95))\n",
    "sfigs = fig.subfigures(1, 3)\n",
    "\n",
    "layout = (3, 3)\n",
    "axsL = sfigs[0].subplots(*layout)\n",
    "axsM = sfigs[1].subplots(*layout)\n",
    "axsR = sfigs[2].subplots(*layout)\n",
    "plot_weights(loadings_analytical, intrinsic_dim, img_size, axs=axsL)\n",
    "plot_weights(weights_unreg, intrinsic_dim, img_size, axs=axsM)\n",
    "plot_weights(loadings_by_ae_reg, intrinsic_dim, img_size, axs=axsR)\n",
    "\n",
    "for sfig, suptitle in zip(sfigs, suptitles):\n",
    "    sfig.supxlabel(suptitle, fontsize=11)\n",
    "    sfig.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "\n",
    "fig.savefig(\"../figures/weights-comparison.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Oct 10 2022, 16:20:20) \n[Clang 14.0.0 (clang-1400.0.29.102)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "6be29a84eb79a9d352d976989e4a991481101e9b7f1904e555bef89c75662f39"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
